{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "lab_fine_tune_partial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Erjian96/BMP-to-YUV/blob/master/lab_fine_tune_partial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXlC-gTXBWST"
      },
      "source": [
        " # Lab:  Transfer Learning with a Pre-Trained Deep Neural Network\n",
        "\n",
        "As we discussed earlier, state-of-the-art neural networks involve millions of parameters that are prohibitively difficult to train from scratch.  In this lab, we will illustrate a powerful technique called *fine-tuning* where we start with a large pre-trained network and then re-train only the final layers to adapt to a new task.  The method is also called *transfer learning* and can produce excellent results on very small datasets with very little computational time.  \n",
        "\n",
        "This lab is based partially on this\n",
        "[excellent blog](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html).  In performing the lab, you will learn to:\n",
        "* Build a custom image dataset\n",
        "* Fine tune the final layers of an existing deep neural network for a new classification task.\n",
        "* Load images with a `DataGenerator`.\n",
        "\n",
        "The lab has two versions:\n",
        "* *CPU version*:  In this version, you use lower resolution images so that the lab can be performed on your laptop.  The resulting accuracy is lower.  The code will also take considerable time to execute.\n",
        "* *GPU version*:  This version uses higher resolution images but requires a GPU instance. See the [notes](../GCP/getting_started.md) on setting up a GPU instance on Google Cloud Platform.  The GPU training is much faster (< 1 minute).  \n",
        "\n",
        "**MS students must complete the GPU version** of this lab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OipTGd7DBWSX"
      },
      "source": [
        "## Create a Dataset\n",
        "\n",
        "In this example, we will try to develop a classifier that can discriminate between two classes:  `cars` and `bicycles`.  One could imagine this type of classifier would be useful in vehicle vision systems.   The first task is to build a dataset.  \n",
        "\n",
        "TODO:  Create training and test datasets with:\n",
        "* 1000 training images of cars\n",
        "* 1000 training images of bicylces\n",
        "* 300 test images of cars\n",
        "* 300 test images of bicylces\n",
        "* The images don't need to be the same size.  But, you can reduce the resolution if you need to save disk space.\n",
        "\n",
        "The images should be organized in the following directory structure:\n",
        "\n",
        "    ./train\n",
        "        /car\n",
        "           car_0000.jpg\n",
        "           car_0001.jpg\n",
        "           ...\n",
        "           car_0999.jpg\n",
        "        /bicycle\n",
        "           bicycle_0000.jpg\n",
        "           bicycle_0001.jpg\n",
        "           ...\n",
        "           bicycle_0999.jpg\n",
        "    ./test\n",
        "        /car\n",
        "           car_1001.jpg\n",
        "           car_1001.jpg\n",
        "           ...\n",
        "           car_1299.jpg\n",
        "        /bicycle\n",
        "           bicycle_1000.jpg\n",
        "           bicycle_1001.jpg\n",
        "           ...\n",
        "           bicycle_1299.jpg\n",
        "           \n",
        "The naming of the files within the directories does not matter.  The `ImageDataGenerator` class below will find the filenames.  Just make sure there are the correct number of files in each directory.\n",
        "           \n",
        "A nice automated way of building such a dataset if through the [FlickrAPI](demo2_flickr_images.ipynb).  Remember that if you run the FlickrAPI twice, it may collect the same images.  So, you need to run it once and split the images into training and test directories.         \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldZSUHmcBWSa"
      },
      "source": [
        "## Loading a Pre-Trained Deep Network\n",
        "\n",
        "We follow the [VGG16 demo](./demo3_vgg16.ipynb) to load a pre-trained deep VGG16 network.  First, run a command to verify your instance is connected to a GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpcnOCRjBWSb",
        "outputId": "f421cc75-dd39-47b7-d9bc-4ee6165bd958",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 577942679329745201\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 15514793777222650248\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 357983292685601492\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 14640891840\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 14533282342300267953\n",
            "physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h2P_zeZBWSi"
      },
      "source": [
        "Now load the appropriate tensorflow packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5axBFFUsBWSj"
      },
      "source": [
        "from tensorflow.keras import applications\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Dense\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMx6zVoqBWSs"
      },
      "source": [
        "We also load some standard packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fc_UBWHBWSt"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIuZSIn6BWSy"
      },
      "source": [
        "Clear the Keras session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCMvOAwsBWSy"
      },
      "source": [
        "import tensorflow.keras.backend as K\n",
        "K.clear_session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiZBa9c8BWTi"
      },
      "source": [
        "Set the dimensions of the input image.  The sizes below would work on a GPU machine.  But, if you have a CPU image, you can use a smaller image size, like `64 x 64`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvXKDq6nBWTj"
      },
      "source": [
        "# TODO:  Set to smaller values if you are using a CPU.  \n",
        "# Otherwise, do not change this code.\n",
        "nrow = 150\n",
        "ncol = 150"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMMQWXPlBWTl"
      },
      "source": [
        "Now we follow the [VGG16 demo](./vgg16.ipynb) and load the deep VGG16 network.  Alternatively, you can use any other pre-trained model in keras.  When using the `applications.VGG16` method you will need to:\n",
        "* Set `include_top=False` to not include the top layer\n",
        "* Set the `image_shape` based on the above dimensions.  Remember, `image_shape` should be `height x width x 3` since the images are color."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzpkMg8jBWTm",
        "outputId": "3cbd94b5-12c6-4b33-ce54-846a5a326570",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TODO:  Load the VGG16 network\n",
        "input_shape = (nrow,ncol,3)\n",
        "base_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=input_shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTkjXWSJBWTo"
      },
      "source": [
        "To create now new model, we create a Sequential model.  Then, loop over the layers in `base_model.layers` and add each layer to the new model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4HusIALBWTo"
      },
      "source": [
        "# Create a new model\n",
        "model = Sequential()\n",
        "\n",
        "# TODO:  Loop over base_model.layers and add each layer to model\n",
        "for layer in base_model.layers:\n",
        "    model.add(layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBbGEWMJBWTq"
      },
      "source": [
        "Next, loop through the layers in `model`, and freeze each layer by setting `layer.trainable = False`.  This way, you will not have to *re-train* any of the existing layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCo0c7B9BWTq"
      },
      "source": [
        "# TODO\n",
        "for layer in model.layers:\n",
        "    layer.trainable=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKVGU_yLBWTs"
      },
      "source": [
        "Now, add the following layers to `model`:\n",
        "* A `Flatten()` layer which reshapes the outputs to a single channel.\n",
        "* A fully-connected layer with 256 output units and `relu` activation\n",
        "* A `Dropout(0.5)` layer.\n",
        "* A final fully-connected layer.  Since this is a binary classification, there should be one output and `sigmoid` activation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6L2yxGGBWTs"
      },
      "source": [
        "# TODO\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oswyjQHzBWTu"
      },
      "source": [
        "Print the model summary.  This will display the number of trainable parameters vs. the non-trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78G1mwvnBWTu",
        "outputId": "4ba09699-c6a2-479e-a201-5fb7410d005e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 8192)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               2097408   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 16,812,353\n",
            "Trainable params: 2,097,665\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXmvcAueBWTw"
      },
      "source": [
        "## Using Generators to Load Data\n",
        "\n",
        "Up to now, the training data has been represented in a large matrix.  This is not possible for image data when the datasets are very large.  For these applications, the `keras` package provides a `ImageDataGenerator` class that can fetch images on the fly from a directory of images.  Using multi-threading, training can be performed on one mini-batch while the image reader can read files for the next mini-batch. The code below creates an `ImageDataGenerator` for the training data.  In addition to the reading the files, the `ImageDataGenerator` creates random deformations of the image to expand the total dataset size.  When the training data is limited, using data augmentation is very important."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iz2-YXPBWTw",
        "outputId": "f5b0fae1-37ff-4973-da33-8c1542da4b89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_data_dir = './train'\n",
        "batch_size = 32\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "                        train_data_dir,\n",
        "                        target_size=(nrow,ncol),\n",
        "                        batch_size=batch_size,\n",
        "                        class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 0 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2r8IoS2jBWTy"
      },
      "source": [
        "Now, create a similar `test_generator` for the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZBnEx9WBWTy",
        "outputId": "413dc975-5da4-4ca2-833a-d4b38480d6cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# TODO\n",
        "test_date_dir = './test'\n",
        "batch_size = 32\n",
        "test_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                  shear_range=0.2,\n",
        "                                  zoom_range=0.2,\n",
        "                                  horizontal_flip=True)                  \n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "                      test_date_dir,\n",
        "                      target_size=(nrow,ncol),\n",
        "                      batch_size=batch_size,\n",
        "                      class_mode='binary')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 0 images belonging to 0 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUP98RrQBWTz"
      },
      "source": [
        "The following function displays images that will be useful below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkPKFXc_BWTz"
      },
      "source": [
        "# Display the image\n",
        "def disp_image(im):\n",
        "    if (len(im.shape) == 2):\n",
        "        # Gray scale image\n",
        "        plt.imshow(im, cmap='gray')    \n",
        "    else:\n",
        "        # Color image.  \n",
        "        im1 = (im-np.min(im))/(np.max(im)-np.min(im))*255\n",
        "        im1 = im1.astype(np.uint8)\n",
        "        plt.imshow(im1)    \n",
        "        \n",
        "    # Remove axis ticks\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYXVueA8BWT1"
      },
      "source": [
        "To see how the `train_generator` works, use the `train_generator.next()` method to get a minibatch of data `X,y`.  Display the first 8 images in this mini-batch and label the image with the class label.  You should see that bicycles have `y=0` and cars have `y=1`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpfQRLnaBWT1",
        "outputId": "b4e17d0d-c44d-4d2e-c260-26bcf256a69a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X,y = train_generator.next()\n",
        "nplot = 8\n",
        "plt.figure(figsize=(20,20))\n",
        "for i in range(nplot):\n",
        "    plt.subplot(1,nplot,i+1)\n",
        "    disp_image(X[i])\n",
        "    plt.title(int(y[i]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-97859a570d59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnplot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdisp_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x1440 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsQTzUdvBWT2"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "Compile the model.  Select the correct `loss` function, `optimizer` and `metrics`.  Remember that we are performing binary classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF7KBTprBWT2"
      },
      "source": [
        "# TODO.\n",
        "model.compile(optimizer=optimizers.Adam(lr=1e-3),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjA7gBg3BWT4"
      },
      "source": [
        "When using an `ImageDataGenerator`, we have to set two parameters manually:\n",
        "* `steps_per_epoch =  training data size // batch_size`\n",
        "* `validation_steps =  test data size // batch_size`\n",
        "\n",
        "We can obtain the training and test data size from `train_generator.n` and `test_generator.n`, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLlcHc6LBWT4"
      },
      "source": [
        "# TODO\n",
        "steps_per_epoch = train_generator.n\n",
        "validation_steps = test_generator.n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJfn1rREBWT5"
      },
      "source": [
        "Now, we run the fit.  If you are using a CPU on a regular laptop, each epoch will take about 3-4 minutes, so you should be able to finish 5 epochs or so within 20 minutes.  On a reasonable GPU, even with the larger images, it will take about 10 seconds per epoch.\n",
        "* If you use `(nrow,ncol) = (64,64)` images, you should get around 90% accuracy after 5 epochs.\n",
        "* If you use `(nrow,ncol) = (150,150)` images, you should get around 96% accuracy after 5 epochs.  But, this will need a GPU.\n",
        "\n",
        "You will get full credit for either version.  With more epochs, you may get slightly higher, but you will have to play with the damping.\n",
        "\n",
        "Remember to record the history of the fit, so that you can plot the training and validation accuracy curve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFXv0VikBWT6",
        "outputId": "30899d17-043b-4641-e10c-e536da7730dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        }
      },
      "source": [
        "nepochs = 5  # Number of epochs\n",
        "\n",
        "# Call the fit_generator function\n",
        "hist = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=nepochs,\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=validation_steps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-18-676af86ec052>:9: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-676af86ec052>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     validation_steps=validation_steps)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m               instructions)\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1828\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1829\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m   @deprecation.deprecated(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1061\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m     \u001b[0;31m# Since we have to know the dtype of the python generator when we build the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m     \u001b[0;31m# dataset, we have to look at a batch to infer the structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m     \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    787\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_peek_and_restore\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    918\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m   def _handle_multiprocessing(self, x, workers, use_multiprocessing,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     55\u001b[0m                              \u001b[0;34m'but the Sequence '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                              'has length {length}'.format(idx=idx,\n\u001b[0;32m---> 57\u001b[0;31m                                                           length=len(self)))\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_batches_seen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Asked to retrieve element 0, but the Sequence has length 0"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0G-anD8BWT7"
      },
      "source": [
        "# Plot the training accuracy and validation accuracy curves on the same figure.\n",
        "\n",
        "# TO DO\n",
        "train_acc = hist.history['accuracy']\n",
        "valid_acc = hist.history['val_accuracy']\n",
        "plt.plot(train_acc)\n",
        "plt.plot(valid_acc)\n",
        "plt.xlabel('epoch')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_ef5R4tBWT9"
      },
      "source": [
        "## Plotting the Error Images\n",
        "\n",
        "Now try to plot some images that were in error:\n",
        "\n",
        "*  Generate a mini-batch `Xts,yts` from the `test_generator.next()` method\n",
        "*  Get the class probabilities using the `model.predict( )` method and compute predicted labels `yhat`.\n",
        "*  Get the images where `yts[i] ~= yhat[i]`.\n",
        "*  If you did not get any prediction error in one minibatch, run it multiple times.\n",
        "*  After you a get a few error images (say 4-8), plot the error images with the true labels and class probabilities predicted by the classifie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3gT672HBWT9"
      },
      "source": [
        "# TO DO\n",
        "Xst,yst = test_generator.next()\n",
        "yhat = model.predict(Xtest)\n",
        "indix_error = np.where(yts!=yhat)[0]\n",
        "plt.figure(figsize=(20,20))\n",
        "number = len(indix_error)\n",
        "if (number>3):\n",
        "    plt.subplot(number,1,i+1)\n",
        "    plt.title('true labels'+str(int()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLPYY_13BWT-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}